{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a8ea65",
   "metadata": {},
   "source": [
    "## Mi Experiencia Adaptando CFR para Nocca-Nocca\n",
    "\n",
    "### El problema que tuve\n",
    "\n",
    "Nocca-Nocca tiene un espacio de estados gigantesco que hizo que CFR estándar se trabara por horas y se quedara sin memoria.\n",
    "\n",
    "**Los problemas específicos que encontré**:\n",
    "- Espacio de estados: millones de estados únicos (tablero 8x5, pilas de 3 niveles, 8 direcciones)\n",
    "- CFR estándar crea un nodo por cada estado que visita → explosión de memoria\n",
    "- Los juegos pueden ser muy largos (más de 100 movimientos)\n",
    "- Factor de ramificación muy alto\n",
    "\n",
    "### Lo que implementé para solucionarlo\n",
    "\n",
    "Basándome en lo que leí en papers, implementé:\n",
    "\n",
    "1. **Abstracción de estados**: Reducir el espacio de estados agrupando estados similares\n",
    "2. **Monte Carlo CFR (MCCFR)**: Muestrear trayectorias en lugar de explorar todo\n",
    "3. **CFR+ con clipping**: Truncar regrets negativos para convergencia más rápida\n",
    "4. **Límites de memoria**: Prevenir explosión de nodos\n",
    "5. **Evaluación heurística**: Para posiciones que no son terminales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from games.tictactoe.tictactoe import TicTacToe\n",
    "from games.nocca_nocca.nocca_nocca import NoccaNocca\n",
    "from games.kuhn.kuhn import KuhnPoker\n",
    "from games.kuhn.kuhn_3player import KuhnPoker3Player\n",
    "from games.leduc.leduc import LeducPoker\n",
    "from agents.counterfactualregret_t import CounterFactualRegret\n",
    "from agents.agent_random import RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915db375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cfr_con_timeout(game, nombre_juego, iteraciones_entrenamiento=10, episodios_prueba=5, timeout_segundos=20, save_agents=True):\n",
    "    \"\"\"Prueba CFR con protección de timeout para evitar cuelgues\"\"\"\n",
    "    \n",
    "    try:\n",
    "        agents = {}\n",
    "        \n",
    "        if 'NoccaNocca' in nombre_juego:\n",
    "            max_depth = 5\n",
    "        elif 'Kuhn' in nombre_juego or 'Leduc' in nombre_juego:\n",
    "            max_depth = 25\n",
    "        else:\n",
    "            max_depth = 15\n",
    "        \n",
    "        for agent_id in game.agents:\n",
    "            agents[agent_id] = CounterFactualRegret(game=game, agent=agent_id, max_depth=max_depth)\n",
    "        \n",
    "        inicio_tiempo = time.time()\n",
    "        print(f\"Entrenando CFR para {nombre_juego} ({iteraciones_entrenamiento} iteraciones, timeout: {timeout_segundos}s)...\")\n",
    "        \n",
    "        for agent in agents.values():\n",
    "            agent.train(niter=iteraciones_entrenamiento, timeout_seconds=timeout_segundos)\n",
    "        \n",
    "        tiempo_entrenamiento = time.time() - inicio_tiempo\n",
    "        nodos_aprendidos = len(agents[game.agents[0]].node_dict)\n",
    "        \n",
    "        if nodos_aprendidos > 10000:\n",
    "            print(f\"MUCHOS: {nodos_aprendidos} nodos creados - posible explosión de estados\")\n",
    "\n",
    "        if save_agents:\n",
    "            os.makedirs('trained_cfr_agents', exist_ok=True)\n",
    "            for agent_id, agent in agents.items():\n",
    "                filepath = f'trained_cfr_agents/{nombre_juego}_{agent_id}.pkl'\n",
    "                agent.save_agent(filepath)\n",
    "        \n",
    "        juegos_exitosos = 0\n",
    "        \n",
    "        for episodio in range(episodios_prueba):\n",
    "            try:\n",
    "                game.reset()\n",
    "                pasos = 0\n",
    "                max_pasos = 50 if 'NoccaNocca' in nombre_juego else 200\n",
    "                \n",
    "                while not game.game_over() and pasos < max_pasos:\n",
    "                    action = agents[game.agent_selection].action()\n",
    "                    game.step(action)\n",
    "                    pasos += 1\n",
    "                \n",
    "                if game.game_over():\n",
    "                    juegos_exitosos += 1\n",
    "                elif pasos >= max_pasos: \n",
    "                    print(f\"Step limit: episode {episodio}, steps: {max_pasos}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Episode error: {episodio}, {e}\")\n",
    "                continue\n",
    "        \n",
    "        tasa_exito = juegos_exitosos / episodios_prueba\n",
    "        \n",
    "        return True, agents, {\n",
    "            'tiempo_entrenamiento': tiempo_entrenamiento,\n",
    "            'nodos_aprendidos': nodos_aprendidos,\n",
    "            'tasa_exito': tasa_exito,\n",
    "            'juegos_exitosos': juegos_exitosos,\n",
    "            'episodios_totales': episodios_prueba\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {nombre_juego}, {e}\")\n",
    "        return False, None, {'error': str(e)}\n",
    "\n",
    "def evaluar_cfr_vs_aleatorio(game, agentes_cfr, episodios=20):\n",
    "    \"\"\"Evalúa agentes CFR entrenados contra agentes aleatorios\"\"\"\n",
    "    \n",
    "    victorias_cfr = 0\n",
    "    empates = 0\n",
    "    \n",
    "    for episodio in range(episodios):\n",
    "        game.reset()\n",
    "        \n",
    "        if episodio % 2 == 0:\n",
    "            agents = {\n",
    "                game.agents[0]: agentes_cfr[game.agents[0]],\n",
    "                game.agents[1]: RandomAgent(game=game, agent=game.agents[1])\n",
    "            }\n",
    "            if len(game.agents) > 2:\n",
    "                agents[game.agents[2]] = RandomAgent(game=game, agent=game.agents[2])\n",
    "            agente_cfr = game.agents[0]\n",
    "        else:\n",
    "            agents = {\n",
    "                game.agents[0]: RandomAgent(game=game, agent=game.agents[0]),\n",
    "                game.agents[1]: agentes_cfr[game.agents[1]]\n",
    "            }\n",
    "            if len(game.agents) > 2:\n",
    "                agents[game.agents[2]] = RandomAgent(game=game, agent=game.agents[2])\n",
    "            agente_cfr = game.agents[1]\n",
    "        \n",
    "        pasos = 0\n",
    "        max_pasos = 50 if 'NoccaNocca' in str(type(game).__name__) else 200\n",
    "        while not game.game_over() and pasos < max_pasos:\n",
    "            try:\n",
    "                action = agents[game.agent_selection].action()\n",
    "                game.step(action)\n",
    "                pasos += 1\n",
    "            except:\n",
    "                available = game.available_actions()\n",
    "                if available:\n",
    "                    action = np.random.choice(available)\n",
    "                    game.step(action)\n",
    "                    pasos += 1\n",
    "        \n",
    "        if game.game_over():\n",
    "            rewards = {agent: game.reward(agent) for agent in game.agents}\n",
    "            reward_cfr = rewards[agente_cfr]\n",
    "            otras_rewards = [rewards[agent] for agent in game.agents if agent != agente_cfr]\n",
    "            \n",
    "            if reward_cfr > max(otras_rewards):\n",
    "                victorias_cfr += 1\n",
    "            elif reward_cfr == max(otras_rewards):\n",
    "                empates += 1\n",
    "    \n",
    "    tasa_victoria = victorias_cfr / episodios\n",
    "    \n",
    "    return {\n",
    "        'victorias_cfr': victorias_cfr,\n",
    "        'empates': empates,\n",
    "        'derrotas': episodios - victorias_cfr - empates,\n",
    "        'tasa_victoria': tasa_victoria,\n",
    "        'episodios_totales': episodios\n",
    "    }\n",
    "\n",
    "def cargar_y_evaluar_agente_guardado(game, nombre_juego, episodios=20):\n",
    "    \"\"\"Carga agentes CFR entrenados desde archivo y los evalúa\"\"\"\n",
    "    \n",
    "    agents_guardados = {}\n",
    "    directorio = 'trained_cfr_agents'\n",
    "    \n",
    "    if not os.path.exists(directorio):\n",
    "        print(f\"Directorio {directorio} no encontrado. No hay agentes guardados.\")\n",
    "        return None\n",
    "        \n",
    "    for agent_id in game.agents:\n",
    "        filepath = f'{directorio}/{nombre_juego}_{agent_id}.pkl'\n",
    "        if os.path.exists(filepath):\n",
    "            agente_cargado = CounterFactualRegret.load_trained_agent(filepath, game, agent_id)\n",
    "            agents_guardados[agent_id] = agente_cargado\n",
    "        else:\n",
    "            print(f\"Archivo {filepath} no encontrado para agente {agent_id}\")\n",
    "            return None\n",
    "            \n",
    "    if len(agents_guardados) != len(game.agents):\n",
    "        print(f\"No se pudieron cargar todos los agentes para {nombre_juego}\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"Agentes CFR cargados exitosamente para {nombre_juego}\")\n",
    "    \n",
    "    resultados = evaluar_cfr_vs_aleatorio(game, agents_guardados, episodios)\n",
    "    \n",
    "    return {\n",
    "        'agentes_cargados': agents_guardados,\n",
    "        'evaluacion': resultados,\n",
    "        'nodos_totales': sum(len(agent.node_dict) for agent in agents_guardados.values())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.counterfactualregret_t import Node\n",
    "\n",
    "class NoccaCFRAgent(CounterFactualRegret):\n",
    "    \"\"\"\n",
    "    CFR adaptado para Nocca-Nocca\n",
    "    \n",
    "    Adaptaciones:\n",
    "    1. Abstracción agresiva de estados\n",
    "    2. Heurísticas de terminación temprana\n",
    "    3. Límites de memoria\n",
    "    4. Función de evaluación simplificada\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, game, agent, max_nodes=800):\n",
    "        super().__init__(game, agent, max_depth=6)  \n",
    "        self.max_nodes = max_nodes\n",
    "        self.abstract_states = {}\n",
    "        \n",
    "    def abstract_game_state(self, game):\n",
    "        \"\"\"\n",
    "        Reduce el espacio de estados de Nocca-Nocca\n",
    "        \n",
    "        En lugar de rastrear posiciones exactas, rastrea:\n",
    "        1. Conteos de piezas\n",
    "        2. Distancia al objetivo (agrupada)  \n",
    "        3. Si alguna pieza está bloqueada\n",
    "        4. Fase del juego\n",
    "        \"\"\"\n",
    "        if game.board is None:\n",
    "            return \"initial\"\n",
    "            \n",
    "        board = game.board.squares\n",
    "        \n",
    "        black_pieces = []\n",
    "        white_pieces = []\n",
    "        \n",
    "        for x in range(8):\n",
    "            for y in range(5):\n",
    "                for z in range(3):\n",
    "                    if board[x, y, z] == 0:\n",
    "                        black_pieces.append((x, y, z))\n",
    "                    elif board[x, y, z] == 1:\n",
    "                        white_pieces.append((x, y, z))\n",
    "        \n",
    "        black_count = len(black_pieces)\n",
    "        white_count = len(white_pieces)\n",
    "        \n",
    "        if black_pieces:\n",
    "            black_min_dist = min([7 - x for x, y, z in black_pieces])\n",
    "            black_dist_bucket = min(black_min_dist // 2, 3)\n",
    "        else:\n",
    "            black_dist_bucket = 4\n",
    "            \n",
    "        if white_pieces:\n",
    "            white_min_dist = min([x for x, y, z in white_pieces]) \n",
    "            white_dist_bucket = min(white_min_dist // 2, 3)\n",
    "        else:\n",
    "            white_dist_bucket = 4\n",
    "        \n",
    "        black_free = any(z == 2 or board[x, y, z+1] == -1 for x, y, z in black_pieces)\n",
    "        white_free = any(z == 2 or board[x, y, z+1] == -1 for x, y, z in white_pieces)\n",
    "        \n",
    "        total_pieces = black_count + white_count\n",
    "        if total_pieces >= 8:\n",
    "            phase = \"early\"\n",
    "        elif total_pieces >= 5:  \n",
    "            phase = \"mid\"\n",
    "        else:\n",
    "            phase = \"late\"\n",
    "        \n",
    "        abstract_state = f\"{phase}_B{black_count}W{white_count}_BD{black_dist_bucket}WD{white_dist_bucket}_BF{int(black_free)}WF{int(white_free)}\"\n",
    "        \n",
    "        return abstract_state\n",
    "    \n",
    "    def observe(self, game):\n",
    "        return self.abstract_game_state(game)\n",
    "    \n",
    "    def cfr_rec(self, game, agent, probability, depth=0):\n",
    "        \"\"\"Override recursión CFR con límites para Nocca-Nocca\"\"\"\n",
    "        \n",
    "        if depth >= self.max_depth:\n",
    "            return self.evaluate_position(game, agent)\n",
    "            \n",
    "        if game.game_over():\n",
    "            return self._safe_get_reward(game, agent)\n",
    "        \n",
    "        if len(self.node_dict) >= self.max_nodes:\n",
    "            return self.evaluate_position(game, agent)\n",
    "            \n",
    "        obs = self.observe(game)\n",
    "        \n",
    "        if obs not in self.node_dict and len(self.node_dict) < self.max_nodes:\n",
    "            self.node_dict[obs] = Node(game, obs)\n",
    "        elif obs not in self.node_dict:\n",
    "            return self.evaluate_position(game, agent)\n",
    "            \n",
    "        node = self.node_dict[obs]\n",
    "        available_actions = game.available_actions()\n",
    "        \n",
    "        if not available_actions:\n",
    "            return self.evaluate_position(game, agent)\n",
    "        \n",
    "        if len(available_actions) > 8:\n",
    "            available_actions = np.random.choice(available_actions, 8, replace=False).tolist()\n",
    "        \n",
    "        if game.agent_selection == agent:\n",
    "            action_utilities = np.zeros(node.num_actions)\n",
    "            \n",
    "            for action in available_actions:\n",
    "                if action < len(action_utilities):\n",
    "                    try:\n",
    "                        game_copy = game.clone()\n",
    "                        game_copy.step(action)\n",
    "                        action_utilities[action] = self.cfr_rec(game_copy, agent, probability, depth + 1)\n",
    "                    except:\n",
    "                        action_utilities[action] = self.evaluate_position(game, agent)\n",
    "            \n",
    "            available_policy = self._get_available_policy(node, available_actions)\n",
    "            node_utility = 0.0\n",
    "            for i, action in enumerate(available_actions):\n",
    "                if action < len(action_utilities):\n",
    "                    node_utility += available_policy[i] * action_utilities[action]\n",
    "            \n",
    "            try:\n",
    "                prob_agent = probability[game.agent_name_mapping[agent]]\n",
    "                utility_for_update = np.zeros(node.num_actions)\n",
    "                for action in available_actions:\n",
    "                    if action < len(utility_for_update):\n",
    "                        utility_for_update[action] = action_utilities[action]\n",
    "                node.update(utility_for_update, node_utility, prob_agent)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return node_utility\n",
    "        else:\n",
    "            available_policy = self._get_available_policy(node, available_actions)\n",
    "            expected_utility = 0.0\n",
    "            \n",
    "            for i, action in enumerate(available_actions):\n",
    "                try:\n",
    "                    game_copy = game.clone()\n",
    "                    game_copy.step(action)\n",
    "                    new_prob = probability.copy()\n",
    "                    other_agent_idx = game.agent_name_mapping[game.agent_selection]\n",
    "                    new_prob[other_agent_idx] *= available_policy[i]\n",
    "                    expected_utility += available_policy[i] * self.cfr_rec(game_copy, agent, new_prob, depth + 1)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return expected_utility\n",
    "    \n",
    "    def evaluate_position(self, game, agent):\n",
    "        \"\"\"\n",
    "        Evaluación heurística para Nocca-Nocca\n",
    "        \n",
    "        Evalúa basado en:\n",
    "        1. Distancia al objetivo\n",
    "        2. Conteo de piezas\n",
    "        3. Movilidad de piezas\n",
    "        \"\"\"\n",
    "        if game.board is None:\n",
    "            return 0.0\n",
    "            \n",
    "        board = game.board.squares\n",
    "        \n",
    "        black_progress = 0\n",
    "        white_progress = 0\n",
    "        black_count = 0\n",
    "        white_count = 0\n",
    "        black_mobile = 0\n",
    "        white_mobile = 0\n",
    "        \n",
    "        for x in range(8):\n",
    "            for y in range(5):\n",
    "                for z in range(3):\n",
    "                    if board[x, y, z] == 0:\n",
    "                        black_count += 1\n",
    "                        black_progress += (7 - x)\n",
    "                        if z == 2 or board[x, y, z+1] == -1:\n",
    "                            black_mobile += 1\n",
    "                    elif board[x, y, z] == 1:\n",
    "                        white_count += 1\n",
    "                        white_progress += x\n",
    "                        if z == 2 or board[x, y, z+1] == -1:\n",
    "                            white_mobile += 1\n",
    "        \n",
    "        max_progress = 7 * 5\n",
    "        black_score = (black_progress / max_progress) + (black_mobile * 0.1) + (black_count * 0.05)\n",
    "        white_score = (white_progress / max_progress) + (white_mobile * 0.1) + (white_count * 0.05)\n",
    "        \n",
    "        if agent == \"Black\":\n",
    "            return black_score - white_score\n",
    "        else:\n",
    "            return white_score - black_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413025f",
   "metadata": {},
   "source": [
    "### Lo que hace mi adaptación\n",
    "\n",
    "Mi CFR adaptado para Nocca-Nocca implementa varias técnicas para manejar el problema del espacio de estados:\n",
    "\n",
    "**Abstracción de estados**: En lugar de tratar cada configuración del tablero como un estado único, agrupa estados similares basándose en características clave como cantidad de piezas, distancia al objetivo, y movilidad.\n",
    "\n",
    "**Límites de memoria**: Evita la explosión de nodos limitando cuántos puede crear (máximo 800).\n",
    "\n",
    "**Evaluación heurística**: Cuando no puede explorar más, evalúa las posiciones usando heurísticas como distancia al objetivo y movilidad de piezas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df78713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba de CFR Adaptado para Nocca-Nocca - VERSIÓN EXTREMADAMENTE LIMITADA\n",
    "\n",
    "class UltraLimitedCFR(CounterFactualRegret):\n",
    "    \"\"\"Versión extremadamente limitada de CFR para Nocca-Nocca\"\"\"\n",
    "    \n",
    "    def __init__(self, game, agent, max_nodes=20):\n",
    "        super().__init__(game, agent, max_depth=1)\n",
    "        self.max_nodes = max_nodes\n",
    "        \n",
    "    def train(self, niter=5, timeout_seconds=10):\n",
    "        \"\"\"Entrenamiento ultra limitado\"\"\"\n",
    "        print(f\"Entrenamiento ultra limitado para {self.agent} (max {niter} iter, {timeout_seconds}s)\")\n",
    "        start_time = time.time()\n",
    "        for i in range(niter):\n",
    "            if time.time() - start_time >= timeout_seconds:\n",
    "                print(f\"  Timeout en iteración {i}\")\n",
    "                break\n",
    "                \n",
    "            if len(self.node_dict) >= self.max_nodes:\n",
    "                print(f\"  Límite de nodos alcanzado: {len(self.node_dict)}\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                super().train(niter=1, timeout_seconds=timeout_seconds)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                break\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Completado en {elapsed:.1f}s con {len(self.node_dict)} nodos\")\n",
    "\n",
    "def test_nocca_cfr_extreme_limits():\n",
    "    \"\"\"Prueba con límites extremos\"\"\"\n",
    "    print(\"PRUEBA CON LÍMITES EXTREMOS PARA NOCCA-NOCCA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        nocca_game = NoccaNocca()\n",
    "        print(\"1. Creando agentes con límites extremos...\")\n",
    "        agents = {}\n",
    "        for agent_id in nocca_game.agents:\n",
    "            agents[agent_id] = UltraLimitedCFR(nocca_game, agent_id)\n",
    "        \n",
    "        print(\"\\n2. Entrenamiento mínimo (2 iteraciones, 5s timeout)...\")\n",
    "        for agent_id, agent in agents.items():\n",
    "            agent.train(niter=2, timeout_seconds=5)\n",
    "        \n",
    "        print(\"\\n3. Probando juego corto...\")\n",
    "        nocca_game.reset()\n",
    "        moves = 0\n",
    "        max_moves = 5\n",
    "        \n",
    "        while not nocca_game.game_over() and moves < max_moves:\n",
    "            current_agent = nocca_game.agent_selection\n",
    "            try:\n",
    "                action = agents[current_agent].action()\n",
    "                print(f\"  {current_agent} (CFR): acción {action}\")\n",
    "            except:\n",
    "                action = np.random.choice(nocca_game.available_actions())\n",
    "                print(f\"  {current_agent} (fallback): acción {action}\")\n",
    "            \n",
    "            nocca_game.step(action)\n",
    "            moves += 1\n",
    "            \n",
    "        print(f\"\\n4. Resultados:\")\n",
    "        print(f\"  Movimientos realizados: {moves}\")\n",
    "        total_nodes = sum(len(agent.node_dict) for agent in agents.values())\n",
    "        print(f\"  Total nodos creados: {total_nodes}\")\n",
    "        print(f\"  ✅ ÉXITO: CFR funcionó sin crash con Nocca-Nocca!\")\n",
    "        \n",
    "        return True, agents, moves\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        return False, None, 0\n",
    "\n",
    "success_extreme, agents_extreme, moves_made = test_nocca_cfr_extreme_limits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d28d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración: CFR Básico Funcional para Nocca-Nocca\n",
    "\n",
    "def demo_cfr_nocca_basic():\n",
    "    \"\"\"Demostración simple que CFR puede funcionar con Nocca-Nocca\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEMOSTRACIÓN: CFR BÁSICO PARA NOCCA-NOCCA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Crear el juego\n",
    "    game = NoccaNocca()\n",
    "    \n",
    "    print(\"1. Inicializando agente CFR con límites extremos...\")\n",
    "    # CFR con límites ultra conservadores\n",
    "    cfr_agent = CounterFactualRegret(game, \"Black\", max_depth=2)  # Profundidad mínima\n",
    "    \n",
    "    print(\"\\n2. Entrenamiento micro (solo 1 iteración, 5 segundos max)...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        cfr_agent.train(niter=1, timeout_seconds=5)\n",
    "        training_time = time.time() - start_time\n",
    "        nodes_created = len(cfr_agent.node_dict)\n",
    "        print(f\"   Entrenamiento completado: {nodes_created} nodos en {training_time:.1f}s\")\n",
    "        \n",
    "        if nodes_created > 0:\n",
    "            print(f\"   ✅ CFR creó nodos sin explotar!\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  No se crearon nodos (posible timeout temprano)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"   Error en entrenamiento después de {training_time:.1f}s: {e}\")\n",
    "    \n",
    "    print(\"\\n3. Verificando que el agente puede dar acciones...\")\n",
    "    try:\n",
    "        game.reset()\n",
    "        if game.agent_selection == \"Black\":\n",
    "            action = cfr_agent.action()\n",
    "            print(f\"   ✅ CFR agent puede dar acción: {action}\")\n",
    "        else:\n",
    "            print(f\"   Agente no activo, pero inicializado correctamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error obteniendo acción: {e}\")\n",
    "    \n",
    "    print(\"\\n4. Conclusiones:\")\n",
    "    print(f\"   - CFR se inicializa correctamente con Nocca-Nocca\")\n",
    "    print(f\"   - Con límites severos (depth=2, timeout=5s), es manejable\")\n",
    "    print(f\"   - Problema: Nocca-Nocca requiere exploración profunda para ser efectivo\")\n",
    "    print(f\"   - Solución: State abstraction y MCCFR son necesarios\")\n",
    "    \n",
    "    return cfr_agent\n",
    "\n",
    "# Ejecutar demostración\n",
    "demo_agent = demo_cfr_nocca_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a9f8e7",
   "metadata": {},
   "source": [
    "### Lo que aprendí con Nocca-Nocca\n",
    "\n",
    "#### ⚠️ **CONFIRMÉ**: Nocca-Nocca es extremadamente complicado para CFR estándar\n",
    "\n",
    "**Lo que vi en mis pruebas**:\n",
    "\n",
    "1. **Problema de rendimiento grave**\n",
    "   - Incluso con `max_depth=3` y `max_nodes=50`, una sola iteración CFR tardaba más de 400 segundos\n",
    "   - El espacio de estados es tan grande que incluso mi abstracción no alcanzaba\n",
    "   - Factor de ramificación extremadamente alto (8 direcciones × 40 posiciones posibles)\n",
    "\n",
    "2. **Límites que necesité para que funcione**\n",
    "   - `max_depth ≤ 2`: Para evitar exploración exponencial\n",
    "   - `timeout ≤ 5s`: Para que no se cuelgue\n",
    "   - `max_nodes ≤ 50`: Para controlar la memoria\n",
    "   - `iteraciones ≤ 1`: Para entrenamiento que termine\n",
    "\n",
    "3. **El trade-off que encontré**\n",
    "   - **Funcionamiento**: CFR puede inicializarse y dar acciones\n",
    "   - **Efectividad**: Con límites tan severos, aprende muy poco\n",
    "   - **Calidad de juego**: Prácticamente aleatorio por las limitaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO FINAL: CFR Trabajando con Nocca-Nocca (Limitaciones Extremas)\n",
    "\n",
    "def final_working_example():\n",
    "    \"\"\"Ejemplo final que demuestra CFR funcionando con Nocca-Nocca\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'max_depth': 1,\n",
    "        'max_iterations': 1,\n",
    "        'timeout_seconds': 3,\n",
    "        'max_game_steps': 10\n",
    "    }\n",
    "    \n",
    "    print(f\"Params: depth={params['max_depth']}, iter={params['max_iterations']}, timeout={params['timeout_seconds']}s\")\n",
    "    \n",
    "    try:\n",
    "        game = NoccaNocca()\n",
    "        agent = CounterFactualRegret(game, \"Black\", max_depth=params['max_depth'])\n",
    "        \n",
    "        start_time = time.time()\n",
    "        agent.train(niter=params['max_iterations'], timeout_seconds=params['timeout_seconds'])\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Training: {training_time:.1f}s, nodes: {len(agent.node_dict)}\")\n",
    "        \n",
    "        game.reset()\n",
    "        moves_made = 0\n",
    "        \n",
    "        while not game.game_over() and moves_made < params['max_game_steps']:\n",
    "            current_player = game.agent_selection\n",
    "            \n",
    "            if current_player == \"Black\":\n",
    "                try:\n",
    "                    action = agent.action()\n",
    "                    print(f\"CFR action: {action}\")\n",
    "                except:\n",
    "                    action = np.random.choice(game.available_actions())\n",
    "                    print(f\"Fallback action: {action}\")\n",
    "            else:\n",
    "                action = np.random.choice(game.available_actions())\n",
    "                print(f\"Random action: {action}\")\n",
    "            \n",
    "            game.step(action)\n",
    "            moves_made += 1\n",
    "        \n",
    "        if game.game_over():\n",
    "            winner = game.check_for_winner()\n",
    "            print(f\"Game completed: {moves_made} moves, winner: {winner}\")\n",
    "        else:\n",
    "            print(f\"Game limited: {moves_made} moves\")\n",
    "        \n",
    "        return True, agent\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False, None\n",
    "\n",
    "final_success, final_agent = final_working_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Intensivo del CFR Adaptado para Nocca-Nocca\n",
    "\n",
    "def entrenar_nocca_cfr_adaptado_intensivo():\n",
    "    \"\"\"Entrenamiento intensivo del CFR adaptado para Nocca-Nocca\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ENTRENAMIENTO INTENSIVO: CFR ADAPTADO PARA NOCCA-NOCCA\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    nocca_game = NoccaNocca()\n",
    "    \n",
    "    config = {\n",
    "        'iteraciones': 1000,\n",
    "        'max_nodes': 2000,\n",
    "        'timeout_minutos': 15,\n",
    "        'episodios_evaluacion': 30\n",
    "    }\n",
    "    \n",
    "    print(\"Configuración del entrenamiento intensivo:\")\n",
    "    for param, valor in config.items():\n",
    "        print(f\"  {param}: {valor}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"1. Creando agentes CFR adaptados...\")\n",
    "    agentes_adaptados = {}\n",
    "    \n",
    "    for agente_id in nocca_game.agents:\n",
    "        print(f\"   Inicializando agente {agente_id}...\")\n",
    "        agentes_adaptados[agente_id] = NoccaCFRAgent(\n",
    "            nocca_game, \n",
    "            agente_id, \n",
    "            max_nodes=config['max_nodes']\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n2. Iniciando entrenamiento ({config['iteraciones']} iteraciones)...\")\n",
    "    tiempo_inicio = time.time()\n",
    "    \n",
    "    try:\n",
    "        for agente_id, agente in agentes_adaptados.items():\n",
    "            print(f\"   Entrenando {agente_id}...\")\n",
    "            agente.train(\n",
    "                niter=config['iteraciones'], \n",
    "                timeout_seconds=config['timeout_minutos'] * 60\n",
    "            )\n",
    "            \n",
    "            nodos_creados = len(agente.node_dict)\n",
    "            print(f\"   --> {agente_id}: {nodos_creados} nodos aprendidos\")\n",
    "            \n",
    "            if nodos_creados > 0:\n",
    "                print(f\"   ✅ {agente_id}: Entrenamiento exitoso\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ {agente_id}: Pocos nodos creados\")\n",
    "        \n",
    "        tiempo_entrenamiento = time.time() - tiempo_inicio\n",
    "        total_nodos = sum(len(agente.node_dict) for agente in agentes_adaptados.values())\n",
    "        \n",
    "        print(f\"\\n3. Entrenamiento completado:\")\n",
    "        print(f\"   Tiempo total: {tiempo_entrenamiento:.1f} segundos\")\n",
    "        print(f\"   Nodos totales: {total_nodos}\")\n",
    "        print(f\"   Velocidad promedio: {config['iteraciones']/tiempo_entrenamiento:.1f} iter/s\")\n",
    "        \n",
    "        print(f\"\\n4. Guardando agentes entrenados...\")\n",
    "        os.makedirs('trained_cfr_agents', exist_ok=True)\n",
    "        \n",
    "        for agente_id, agente in agentes_adaptados.items():\n",
    "            nombre_archivo = f'NoccaNocca_Adaptado_{agente_id}.pkl'\n",
    "            ruta_archivo = f'trained_cfr_agents/{nombre_archivo}'\n",
    "            agente.save_agent(ruta_archivo)\n",
    "            print(f\"   ✅ Guardado: {nombre_archivo}\")\n",
    "        \n",
    "        print(f\"\\n5. Evaluando rendimiento contra agentes aleatorios...\")\n",
    "        resultado_evaluacion = evaluar_cfr_vs_aleatorio(\n",
    "            nocca_game, \n",
    "            agentes_adaptados, \n",
    "            episodios=config['episodios_evaluacion']\n",
    "        )\n",
    "        \n",
    "        tasa_victoria = resultado_evaluacion['tasa_victoria']\n",
    "        victorias = resultado_evaluacion['victorias_cfr']\n",
    "        total_episodios = resultado_evaluacion['episodios_totales']\n",
    "        \n",
    "        print(f\"\\n6. Resultados de la evaluación:\")\n",
    "        print(f\"   Victorias CFR: {victorias}/{total_episodios}\")\n",
    "        print(f\"   Tasa de victoria: {tasa_victoria:.1%}\")\n",
    "        print(f\"   Empates: {resultado_evaluacion['empates']}\")\n",
    "        print(f\"   Derrotas: {resultado_evaluacion['derrotas']}\")\n",
    "        \n",
    "        print(f\"\\n7. Análisis de la calidad del aprendizaje:\")\n",
    "        \n",
    "        if tasa_victoria >= 0.7:\n",
    "            print(f\"   🟢 EXCELENTE: {tasa_victoria:.1%} - CFR adaptado muy efectivo\")\n",
    "        elif tasa_victoria >= 0.55:\n",
    "            print(f\"   🟡 BUENO: {tasa_victoria:.1%} - CFR adaptado funcionando bien\")\n",
    "        elif tasa_victoria >= 0.45:\n",
    "            print(f\"   🟠 REGULAR: {tasa_victoria:.1%} - CFR adaptado básico\")\n",
    "        else:\n",
    "            print(f\"   🔴 BAJO: {tasa_victoria:.1%} - Necesita más entrenamiento\")\n",
    "        \n",
    "        print(f\"\\n8. Estadísticas de abstracción de estados:\")\n",
    "        for agente_id, agente in agentes_adaptados.items():\n",
    "            estados_unicos = len(set(agente.node_dict.keys()))\n",
    "            print(f\"   {agente_id}: {estados_unicos} estados abstractos únicos\")\n",
    "        \n",
    "        return True, agentes_adaptados, {\n",
    "            'tiempo_entrenamiento': tiempo_entrenamiento,\n",
    "            'total_nodos': total_nodos,\n",
    "            'evaluacion': resultado_evaluacion,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        tiempo_entrenamiento = time.time() - tiempo_inicio\n",
    "        print(f\"\\n❌ ERROR durante entrenamiento después de {tiempo_entrenamiento:.1f}s:\")\n",
    "        print(f\"   {e}\")\n",
    "        return False, None, {'error': str(e), 'tiempo': tiempo_entrenamiento}\n",
    "\n",
    "\n",
    "exito, agentes_nocca, stats_nocca = entrenar_nocca_cfr_adaptado_intensivo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f90f94",
   "metadata": {},
   "source": [
    "## MI CONCLUSIÓN: CFR No Es la Herramienta Correcta para Nocca-Nocca\n",
    "\n",
    "### El Problema Fundamental que Encontré\n",
    "\n",
    "**CFR no está diseñado para juegos como Nocca-Nocca**. Mis datos lo confirman:\n",
    "\n",
    "1. **Información Perfecta**: CFR fue creado para poker (información imperfecta)\n",
    "2. **Espacio de Estados Masivo**: 8×5×3 = 120 posiciones × múltiples configuraciones\n",
    "3. **Juegos Largos**: 100+ movimientos vs 3-10 en poker\n",
    "4. **Factor de Ramificación Alto**: 8 direcciones × múltiples piezas = 50+ acciones por turno\n",
    "\n",
    "### Por Qué Mis \"Mejoras\" No Funcionaron\n",
    "\n",
    "**Los resultados muestran que incluso con optimizaciones avanzadas**:\n",
    "- Win rate sigue siendo ~5% (apenas mejor que aleatorio)\n",
    "- Velocidad sigue siendo lenta (<5 iter/s)\n",
    "- Nodos creados siguen siendo pocos (<100)\n",
    "- Tiempo de entrenamiento sigue siendo excesivo (>20 minutos)\n",
    "\n",
    "### Mi Conclusión Honesta\n",
    "\n",
    "CFR puede funcionar en Nocca-Nocca, pero es muy mala herramienta para este problema. Terminé no usando nada de esto."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
